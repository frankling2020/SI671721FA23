{"cells":[{"cell_type":"markdown","metadata":{"id":"Z4dP6eOw-hjS"},"source":["# Lab 1: Introduction to Machine Learning with scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"twB7Tj9I-hjV"},"source":["scikit-learn is an open source Python machine learning module. Using scikit-learn makes it easy to implement a variety of machine learning tasks including regression, classification, clustering, dimensionality reduction, model selection and data pre-processing. The full documentation for scikit-learn is located at https://scikit-learn.org/stable/. It contains numerous examples and detailed descriptions for each function (and its associated parameters). This notebook will give you a brief introduction on how to implement the basic machine learning pipeline."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yHzju7J8-hjV","executionInfo":{"status":"ok","timestamp":1694012749805,"user_tz":240,"elapsed":23761,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7013df2-696f-4e78-e13b-9ad3fcc6a23b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn==1.1.3\n","  Using cached scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.10.1)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.2.0)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","Successfully installed scikit-learn-1.1.3\n"]}],"source":["# if you are missing any of the packages, uncomment the line(s) below to install\n","# %pip install sklearn\n","# %pip install pandas\n","# %pip install numpy\n","%pip install scikit-learn==1.1.3"]},{"cell_type":"markdown","metadata":{"id":"7l39GbGi-hjW"},"source":["### Loading & Previewing Data"]},{"cell_type":"markdown","metadata":{"id":"ZLwcRFhb-hjW"},"source":["The scikit-learn library contains a few small datasets that we can experiment with. Descriptions of the datasets are included here: https://scikit-learn.org/stable/datasets/toy_dataset.html. We will use two of them in this tutorial:\n","\n","1) The Boston house prices dataset (for regression tasks) and\n","\n","2) The breast cancer wisconsin (diagnostic) dataset (for classification tasks)"]},{"cell_type":"markdown","metadata":{"id":"u144D3CK-hjX"},"source":["First we will load the datasets using helper functions and will take a look at the contents:"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-rmC7l8-hjX","executionInfo":{"status":"ok","timestamp":1694013266580,"user_tz":240,"elapsed":126,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"2bd6b380-fd3c-4bf5-e775-8a5d4ee6f9f4"},"outputs":[{"output_type":"stream","name":"stdout","text":[".. _boston_dataset:\n","\n","Boston house prices dataset\n","---------------------------\n","\n","**Data Set Characteristics:**  \n","\n","    :Number of Instances: 506 \n","\n","    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n","\n","    :Attribute Information (in order):\n","        - CRIM     per capita crime rate by town\n","        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n","        - INDUS    proportion of non-retail business acres per town\n","        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","        - NOX      nitric oxides concentration (parts per 10 million)\n","        - RM       average number of rooms per dwelling\n","        - AGE      proportion of owner-occupied units built prior to 1940\n","        - DIS      weighted distances to five Boston employment centres\n","        - RAD      index of accessibility to radial highways\n","        - TAX      full-value property-tax rate per $10,000\n","        - PTRATIO  pupil-teacher ratio by town\n","        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n","        - LSTAT    % lower status of the population\n","        - MEDV     Median value of owner-occupied homes in $1000's\n","\n","    :Missing Attribute Values: None\n","\n","    :Creator: Harrison, D. and Rubinfeld, D.L.\n","\n","This is a copy of UCI ML housing dataset.\n","https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n","\n","\n","This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n","\n","The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n","prices and the demand for clean air', J. Environ. Economics & Management,\n","vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n","...', Wiley, 1980.   N.B. Various transformations are used in the table on\n","pages 244-261 of the latter.\n","\n","The Boston house-price data has been used in many machine learning papers that address regression\n","problems.   \n","     \n",".. topic:: References\n","\n","   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n","   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n","\n","    The Boston housing prices dataset has an ethical problem. You can refer to\n","    the documentation of this function for further details.\n","\n","    The scikit-learn maintainers therefore strongly discourage the use of this\n","    dataset unless the purpose of the code is to study and educate about\n","    ethical issues in data science and machine learning.\n","\n","    In this special case, you can fetch the dataset from the original\n","    source::\n","\n","        import pandas as pd\n","        import numpy as np\n","\n","        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","        target = raw_df.values[1::2, 2]\n","\n","    Alternative datasets include the California housing dataset (i.e.\n","    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n","    dataset. You can load the datasets as follows::\n","\n","        from sklearn.datasets import fetch_california_housing\n","        housing = fetch_california_housing()\n","\n","    for the California housing dataset and::\n","\n","        from sklearn.datasets import fetch_openml\n","        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n","\n","    for the Ames housing dataset.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["from sklearn.datasets import load_boston\n","import pandas as pd\n","\n","boston = load_boston()\n","print(boston.DESCR)\n","(boston_X, boston_y) = load_boston(return_X_y=True)"]},{"cell_type":"markdown","source":["### How to drop the column 'B' in the dataset"],"metadata":{"id":"uttWHHpNoihu"}},{"cell_type":"code","source":["import numpy as np\n","boston_X = np.delete(boston_X, np.where(boston.feature_names == 'B')[0], axis=1)\n","boston_sub_feats_name = boston.feature_names[boston.feature_names != 'B']\n","boston_X_df = pd.DataFrame(boston_X, columns=boston_sub_feats_name)\n","boston_X_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"VrWrcXmWl-WL","executionInfo":{"status":"ok","timestamp":1694013267563,"user_tz":240,"elapsed":297,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"58ccb6a6-d220-44a4-8712-4d9c87a2e05a"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n","0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n","1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n","2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n","3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n","4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n","\n","   PTRATIO  LSTAT  \n","0     15.3   4.98  \n","1     17.8   9.14  \n","2     17.8   4.03  \n","3     18.7   2.94  \n","4     18.7   5.33  "],"text/html":["\n","  <div id=\"df-fcfe3ef1-40e9-44be-a0a5-a311e29b7f8c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CRIM</th>\n","      <th>ZN</th>\n","      <th>INDUS</th>\n","      <th>CHAS</th>\n","      <th>NOX</th>\n","      <th>RM</th>\n","      <th>AGE</th>\n","      <th>DIS</th>\n","      <th>RAD</th>\n","      <th>TAX</th>\n","      <th>PTRATIO</th>\n","      <th>LSTAT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00632</td>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0.0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1.0</td>\n","      <td>296.0</td>\n","      <td>15.3</td>\n","      <td>4.98</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02731</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0.0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2.0</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>9.14</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.02729</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0.0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2.0</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>4.03</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03237</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0.0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3.0</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>2.94</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.06905</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0.0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3.0</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>5.33</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcfe3ef1-40e9-44be-a0a5-a311e29b7f8c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fcfe3ef1-40e9-44be-a0a5-a311e29b7f8c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fcfe3ef1-40e9-44be-a0a5-a311e29b7f8c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b224300f-b5b6-426e-80cf-540a1d6f584b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b224300f-b5b6-426e-80cf-540a1d6f584b')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b224300f-b5b6-426e-80cf-540a1d6f584b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7kBSq2B-hjY","executionInfo":{"status":"ok","timestamp":1694013268224,"user_tz":240,"elapsed":103,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"f6bb50ab-4bba-446e-cc54-f84d55f5f4cf"},"outputs":[{"output_type":"stream","name":"stdout","text":[".. _breast_cancer_dataset:\n","\n","Breast cancer wisconsin (diagnostic) dataset\n","--------------------------------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 569\n","\n","    :Number of Attributes: 30 numeric, predictive attributes and the class\n","\n","    :Attribute Information:\n","        - radius (mean of distances from center to points on the perimeter)\n","        - texture (standard deviation of gray-scale values)\n","        - perimeter\n","        - area\n","        - smoothness (local variation in radius lengths)\n","        - compactness (perimeter^2 / area - 1.0)\n","        - concavity (severity of concave portions of the contour)\n","        - concave points (number of concave portions of the contour)\n","        - symmetry\n","        - fractal dimension (\"coastline approximation\" - 1)\n","\n","        The mean, standard error, and \"worst\" or largest (mean of the three\n","        worst/largest values) of these features were computed for each image,\n","        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n","        10 is Radius SE, field 20 is Worst Radius.\n","\n","        - class:\n","                - WDBC-Malignant\n","                - WDBC-Benign\n","\n","    :Summary Statistics:\n","\n","    ===================================== ====== ======\n","                                           Min    Max\n","    ===================================== ====== ======\n","    radius (mean):                        6.981  28.11\n","    texture (mean):                       9.71   39.28\n","    perimeter (mean):                     43.79  188.5\n","    area (mean):                          143.5  2501.0\n","    smoothness (mean):                    0.053  0.163\n","    compactness (mean):                   0.019  0.345\n","    concavity (mean):                     0.0    0.427\n","    concave points (mean):                0.0    0.201\n","    symmetry (mean):                      0.106  0.304\n","    fractal dimension (mean):             0.05   0.097\n","    radius (standard error):              0.112  2.873\n","    texture (standard error):             0.36   4.885\n","    perimeter (standard error):           0.757  21.98\n","    area (standard error):                6.802  542.2\n","    smoothness (standard error):          0.002  0.031\n","    compactness (standard error):         0.002  0.135\n","    concavity (standard error):           0.0    0.396\n","    concave points (standard error):      0.0    0.053\n","    symmetry (standard error):            0.008  0.079\n","    fractal dimension (standard error):   0.001  0.03\n","    radius (worst):                       7.93   36.04\n","    texture (worst):                      12.02  49.54\n","    perimeter (worst):                    50.41  251.2\n","    area (worst):                         185.2  4254.0\n","    smoothness (worst):                   0.071  0.223\n","    compactness (worst):                  0.027  1.058\n","    concavity (worst):                    0.0    1.252\n","    concave points (worst):               0.0    0.291\n","    symmetry (worst):                     0.156  0.664\n","    fractal dimension (worst):            0.055  0.208\n","    ===================================== ====== ======\n","\n","    :Missing Attribute Values: None\n","\n","    :Class Distribution: 212 - Malignant, 357 - Benign\n","\n","    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n","\n","    :Donor: Nick Street\n","\n","    :Date: November, 1995\n","\n","This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n","https://goo.gl/U2Uwz2\n","\n","Features are computed from a digitized image of a fine needle\n","aspirate (FNA) of a breast mass.  They describe\n","characteristics of the cell nuclei present in the image.\n","\n","Separating plane described above was obtained using\n","Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n","Construction Via Linear Programming.\" Proceedings of the 4th\n","Midwest Artificial Intelligence and Cognitive Science Society,\n","pp. 97-101, 1992], a classification method which uses linear\n","programming to construct a decision tree.  Relevant features\n","were selected using an exhaustive search in the space of 1-4\n","features and 1-3 separating planes.\n","\n","The actual linear program used to obtain the separating plane\n","in the 3-dimensional space is that described in:\n","[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n","Programming Discrimination of Two Linearly Inseparable Sets\",\n","Optimization Methods and Software 1, 1992, 23-34].\n","\n","This database is also available through the UW CS ftp server:\n","\n","ftp ftp.cs.wisc.edu\n","cd math-prog/cpo-dataset/machine-learn/WDBC/\n","\n",".. topic:: References\n","\n","   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n","     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n","     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n","     San Jose, CA, 1993.\n","   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n","     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n","     July-August 1995.\n","   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n","     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n","     163-171.\n"]}],"source":["from sklearn.datasets import load_breast_cancer\n","\n","cancer = load_breast_cancer()\n","print(cancer.DESCR)\n","(cancer_X, cancer_y) = load_breast_cancer(return_X_y=True)"]},{"cell_type":"markdown","metadata":{"id":"_PcG2hFD-hjY"},"source":["### Splitting Data"]},{"cell_type":"markdown","metadata":{"id":"vl-caOVn-hjZ"},"source":["Before interacting with our datasets any further, we must split the data into training and testing data. This way we can use the training data to explore the data and fit our models and then use the testing data to provide an unbiased measure of the performance of our models. This can be done using the train_test_split function in scikit-learn."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCjQE-Hj-hjZ","executionInfo":{"status":"ok","timestamp":1694013269172,"user_tz":240,"elapsed":117,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"3d0c11c8-1df0-4354-ed6e-27ae400886f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["(379, 12)\n","(127, 12)\n","(379,)\n","(127,)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# split the data with a 75%-25% training-test split\n","# set the random state for reproducible results\n","\n","boston_X_train, boston_X_test, boston_y_train, boston_y_test = train_test_split(\n","                                boston_X, boston_y, test_size=0.25, random_state=671)\n","#checks\n","print(boston_X_train.shape)\n","print(boston_X_test.shape)\n","print(boston_y_train.shape)\n","print(boston_y_test.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"0atKsqm1-hjZ","executionInfo":{"status":"ok","timestamp":1694013269819,"user_tz":240,"elapsed":1,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"outputs":[],"source":["### YOUR CODE: split the breast cancer data with a train-test split of 70%-30% & random_state of 671 into:\n","# (1) cancer_X_train\n","# (2) cancer_X_test\n","# (3) cancer_y_train\n","# (4) cancer_y_test\n"]},{"cell_type":"code","source":["### EXAMPLE ANSWER\n","\n","cancer_X_train, cancer_X_test, cancer_y_train, cancer_y_test = train_test_split(\n","                                cancer_X, cancer_y, test_size=0.30, random_state=671)\n","#checks\n","print(cancer_X_train.shape)\n","print(cancer_X_test.shape)\n","print(cancer_y_train.shape)\n","print(cancer_y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBP28ZHrCNx0","executionInfo":{"status":"ok","timestamp":1694013270441,"user_tz":240,"elapsed":4,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"dd1a86c8-b3c2-406c-b18d-84d6f1b185fe"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["(398, 30)\n","(171, 30)\n","(398,)\n","(171,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"90Fvv_gR-hjZ"},"source":["### Preprocessing the Data"]},{"cell_type":"markdown","metadata":{"id":"U4rfzART-hjZ"},"source":["For almost all datasets, we will need to preprocess the data before we can fit any models. This may involve imputing missing data, scaling our features, applying one-hot encoding, etc."]},{"cell_type":"markdown","metadata":{"id":"IP1QNOe--hjZ"},"source":["#### 1. Imputing Missing Data"]},{"cell_type":"markdown","metadata":{"id":"8FKJkgQ2-hjZ"},"source":["Missing data is a very common problem across all datasets. One simple strategy for addressing it is to impute missing values using a chosen strategy such as the \"mean\", \"median\" or \"most_frequent\". First, let's check for missing data in our datasets."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QOekjuv-hja","executionInfo":{"status":"ok","timestamp":1694013272835,"user_tz":240,"elapsed":229,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"0da69c97-0ef5-4276-d583-c3f1e6a198d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}],"source":["boston_X_train_df = pd.DataFrame(boston_X_train, columns=boston_sub_feats_name)\n","# check for which columns have missing values\n","columns = boston_X_train_df.columns[boston_X_train_df.isnull().any()]\n","print(len(columns))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOwSmvUa-hja","executionInfo":{"status":"ok","timestamp":1694013302150,"user_tz":240,"elapsed":121,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"1d4ab438-3c9d-4940-e3b9-18c9c9a16b6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}],"source":["cancer_X_train_df = pd.DataFrame(cancer_X_train, columns=cancer.feature_names)\n","# check for which columns have missing values\n","columns = cancer_X_train_df.columns[cancer_X_train_df.isnull().any()]\n","print(len(columns))"]},{"cell_type":"markdown","metadata":{"id":"HEj6-d3j-hja"},"source":["As these are \"toy\" datasets, they do not have any missing values but let us still practice how we would impute missing values anyways."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"F5W7j1kC-hja","executionInfo":{"status":"ok","timestamp":1694013338415,"user_tz":240,"elapsed":253,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","import numpy as np\n","\n","imp = SimpleImputer(missing_values=np.nan, strategy='mean') # impute using the 'mean' for the feature\n","boston_X_train_imp = imp.fit_transform(boston_X_train) # use fit_transform on training data\n","boston_X_test_imp = imp.transform(boston_X_test) # use transform on testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daQ4A05_-hja"},"outputs":[],"source":["### YOUR CODE: apply the SimpleImputer with a 'median' strategy to the cancer data\n","### As a result, you should have two variables:\n","#(1) cancer_X_train_imp and\n","#(2) cancer_X_test_imp\n"]},{"cell_type":"code","source":["### EXAMPLE ANSWER\n","\n","imp = SimpleImputer(missing_values=np.nan, strategy='median')\n","cancer_X_train_imp = imp.fit_transform(cancer_X_train)\n","cancer_X_test_imp = imp.transform(cancer_X_test)"],"metadata":{"id":"S-k4eIsRrHC7","executionInfo":{"status":"ok","timestamp":1694013391929,"user_tz":240,"elapsed":132,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Up4-BXPe-hja"},"source":["#### 2.  One-Hot Encoding"]},{"cell_type":"markdown","metadata":{"id":"zHavnVdO-hja"},"source":["Depending on the nature of our data and the ML models we want to implement, we may need to one-hot encode the categorical variables in our data as some models do not support categorical inputs. This means we will transform the feature into a set of dummy variables.\n","\n","Although we do not have any categorical variables in our cancer dataset, we do have a variable, 'RAD', in our boston dataset that we want to one-hot encode as it represents an index of accessibility to radial highways. We can do so using a OneHotEncoder and ColumnTransfer, which allows us to only apply the OneHotEncoder to only the 'RAD' column.\n","\n","First, let's implement the OneHotEncoder with the default parameters."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWPgJeTo-hja","executionInfo":{"status":"ok","timestamp":1694013499449,"user_tz":240,"elapsed":128,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"1dfaf488-1c81-4c24-f792-627d2ff3dd88"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('RAD', OneHotEncoder(), [8]), ('remainder', 'passthrough', [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11])]\n","(379, 20)\n","(127, 20)\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","rad_idx = list(boston.feature_names).index('RAD') # find the index of the 'RAD' column\n","enc_boston = ColumnTransformer([(\"RAD\", OneHotEncoder(),[rad_idx])], remainder=\"passthrough\") # passthrough specifies to keep other columns as they are\n","boston_X_train_enc = enc_boston.fit_transform(boston_X_train_imp) # use fit_transform on training data\n","boston_X_test_enc = enc_boston.transform(boston_X_test_imp) # use transform on testing data\n","print(enc_boston.transformers_)\n","\n","print(boston_X_train_enc.shape)\n","print(boston_X_test_enc.shape)"]},{"cell_type":"markdown","metadata":{"id":"sgDoG12k-hja"},"source":["We can see that after applying the OneHotEncoder to the 'RAD' column, we have increased the number of columns in our dataset from 13 to 21 as it added one column for each unique value of 'RAD'. However, if we use this, we will run into the issue of multicollinearity since each dummy variable can be represented as a linear combination of the other dummy variables. To solve this issue, we want to create n-1 dummy variables. This can be done using the 'drop' parameter of the OneHotEncoder function, as shown below, resulting in one less column."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ie0pwo5F-hja","executionInfo":{"status":"ok","timestamp":1694013575520,"user_tz":240,"elapsed":116,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"d9f98c86-7e47-4ced-889a-e6e05fcbcd4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('RAD', OneHotEncoder(drop='first'), [8]), ('remainder', 'passthrough', [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11])]\n","(379, 19)\n","(127, 19)\n"]}],"source":["enc_boston = ColumnTransformer([(\"RAD\", OneHotEncoder(drop='first'),[rad_idx])], remainder=\"passthrough\")\n","boston_X_train_enc = enc_boston.fit_transform(boston_X_train_imp)\n","boston_X_test_enc = enc_boston.transform(boston_X_test_imp)\n","print(enc_boston.transformers_)\n","\n","print(boston_X_train_enc.shape)\n","print(boston_X_test_enc.shape)"]},{"cell_type":"markdown","metadata":{"id":"zLIaURxi-hjb"},"source":["#### 3. Feature Scaling"]},{"cell_type":"markdown","metadata":{"id":"9Cuf-3vL-hjb"},"source":["For some machine learning models, we must first scale the features so that they are all on a shared scale. This supports faster model convergence and removes any bias toward features with higher magnitudes (i.e., giving more importance to features on a scale of cm vs. inches).\n","\n","\n","Two of the most common techniques for feature normalization are 1) StandardScaler and 2) MinMaxScaler. StandardScaler works by adjusting the mean of each feature to zero with a standard deviation of 1. MinMaxScaler works by scaling all values to between 0 and 1. There are no hard rules for when to use one over the other but some factors to consider are the problem we intend to solve, assumptions regarding the distribution of the data (including the presence of outliers) and the ML models we plan on implementing. It can also be a good option to try out both and see which results in better performance. Either way, we must fit the scaler on our training data and then transform our testing data using it to avoid any data leakage."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"x9eWy9lB-hjb","executionInfo":{"status":"ok","timestamp":1694013659593,"user_tz":240,"elapsed":140,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","boston_X_train_scal = scaler.fit_transform(boston_X_train_enc) # use fit_transform on training data\n","boston_X_test_scal = scaler.transform(boston_X_test_enc) # use transform on testing data"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"wIu2H8xT-hjb","executionInfo":{"status":"ok","timestamp":1694014726481,"user_tz":240,"elapsed":3,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","### YOUR CODE: apply MinMaxScaler to the cancer dataset ('cancer_X_train_imp' and 'cancer_X_test_imp')\n","### name the resulting variables 'cancer_X_train_scal' and 'cancer_X_test_scal'\n"]},{"cell_type":"code","source":["### EXAMPLE ANSWER\n","\n","scaler = MinMaxScaler()\n","cancer_X_train_scal = scaler.fit_transform(cancer_X_train_imp)\n","cancer_X_test_scal = scaler.transform(cancer_X_test_imp)"],"metadata":{"id":"HJsheXrLKmQQ","executionInfo":{"status":"ok","timestamp":1694014727357,"user_tz":240,"elapsed":124,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hIKxVv39-hjb"},"source":["### Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"NTll4kMS-hjb"},"source":["When we have large datasets with high dimensionality, feature selection can help us reduce the dimensionality (e.g., the number of input variables or features) by removing irrelevant or redundant features. This can help with reducing the computational costs associated with training models and can also improve the performance of our models in some cases. There are many feature selection techniques so we will only experiment with a few here."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wsiw0NQf-hjb","executionInfo":{"status":"ok","timestamp":1694013928007,"user_tz":240,"elapsed":261,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"dd4ffeae-0688-43a0-b31c-32c82afe4554"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape (after applying Mutual Info): (379, 10)\n","Testing data shape (after applying Mutual Info): (127, 10)\n","Training shape (after applying PCA): (379, 11)\n","Testing shape (after applying PCA): (127, 11)\n"]}],"source":["# Method 1: Mutual Information (regression)\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import mutual_info_regression\n","\n","selector = SelectKBest(mutual_info_regression, k = 10) # select the top 10 features\n","boston_X_train_mi = selector.fit_transform(boston_X_train_scal, boston_y_train)\n","print(\"Training data shape (after applying Mutual Info):\", boston_X_train_mi.shape)\n","boston_X_test_mi = selector.transform(boston_X_test_scal)\n","print(\"Testing data shape (after applying Mutual Info):\", boston_X_test_mi.shape)\n","\n","# Method 2: Principal component analysis (PCA)\n","from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components = 0.90) # can specify either the percent of explained variance or number of features\n","boston_X_train_pca = pca.fit_transform(boston_X_train_scal, boston_y_train)\n","print(\"Training shape (after applying PCA):\", boston_X_train_pca.shape)\n","boston_X_test_pca = pca.transform(boston_X_test_scal)\n","print(\"Testing shape (after applying PCA):\", boston_X_test_pca.shape)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"bVuOSdGA-hjb","executionInfo":{"status":"ok","timestamp":1694014741414,"user_tz":240,"elapsed":125,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"outputs":[],"source":["# YOUR CODE: apply Mutual Information (classification) to the breast cancer data\n","# select the top 15 features and name the resulting variables 'cancer_X_train_mi' & 'cancer_X_test_mi'\n","\n","from sklearn.feature_selection import mutual_info_classif\n"]},{"cell_type":"code","source":["### EXAMPLE ANSWER\n","\n","selector = SelectKBest(mutual_info_classif, k = 15)  # select the top 15 features\n","cancer_X_train_mi = selector.fit_transform(cancer_X_train_scal, cancer_y_train)\n","print(\"Training shape (after applying Mutual Info):\", cancer_X_train_mi.shape)\n","cancer_X_test_mi = selector.transform(cancer_X_test_scal)\n","print(\"Testing shape (after applying Mutual Info):\", cancer_X_test_mi.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x96LNdp-K8HG","executionInfo":{"status":"ok","timestamp":1694014742177,"user_tz":240,"elapsed":147,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"00009ba2-a4f6-4ded-b6b5-1ed0cc6ff0f7"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Training shape (after applying Mutual Info): (398, 15)\n","Testing shape (after applying Mutual Info): (171, 15)\n"]}]},{"cell_type":"markdown","metadata":{"id":"AcuKsOwZ-hjb"},"source":["### Model Fitting, Cross-Validation & Evaluation"]},{"cell_type":"markdown","metadata":{"id":"NhucrX_f-hjb"},"source":["Now we can finally fit some models! scikit-learn allows you to easily implement a variety of models. We will work with a few of them here."]},{"cell_type":"markdown","metadata":{"id":"3N0v0el7-hjb"},"source":["Let's start by fitting two models - (1) Linear Regression and (2) K-Nearest Neighbors Regression - to our boston housing dataset. We will fit the models to each of the three feature sets - (1) all of the features (no feature selection), (2) the feature subset selected using mutual information and (3) the feature subset selected using PCA so that we can compare performance both between the models and the different feature subsets. We will use cross-validation (3 folds in this example) to get a more reliable estimate of the performance of our models without having to touch our test dataset yet. The default performance metric will be $R^2$, or the proportion of explained variance."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2lYGqoA-hjb","executionInfo":{"status":"ok","timestamp":1694014373892,"user_tz":240,"elapsed":342,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"88b6fa4b-24a6-4e15-848c-91d6b1f08be8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Linear Regression:\n","                      R^2 on Fold 1  R^2 on Fold 2  R^2 on Fold 3  Mean R^2\n","No Feature Selection       0.776987       0.650943       0.671001  0.699644\n","MI Features                0.750709       0.670249       0.651748  0.690902\n","PCA Features               0.797050       0.584203       0.614066  0.665106\n","\n","K-Nearest Neighbors Regression:\n","                      R^2 on Fold 1  R^2 on Fold 2  R^2 on Fold 3  Mean R^2\n","No Feature Selection       0.753978       0.532383       0.620133  0.635498\n","MI Features                0.845605       0.792886       0.722900  0.787130\n","PCA Features               0.739534       0.478911       0.579351  0.599265\n"]}],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","lr = LinearRegression()\n","\n","lr_scores_df = pd.DataFrame(columns=[\"R^2 on Fold 1\", \"R^2 on Fold 2\", \"R^2 on Fold 3\", \"Mean R^2\"])\n","print(\"Linear Regression:\")\n","lr_scal_scores = list(cross_val_score(lr, boston_X_train_scal, boston_y_train, cv=3))\n","lr_scal_scores.append(np.mean(lr_scal_scores))\n","lr_scores_df.loc[\"No Feature Selection\"] = lr_scal_scores\n","lr_mi_scores = list(cross_val_score(lr, boston_X_train_mi, boston_y_train, cv=3))\n","lr_mi_scores.append(np.mean(lr_mi_scores))\n","lr_scores_df.loc[\"MI Features\"] = lr_mi_scores\n","lr_pca_scores = list(cross_val_score(lr, boston_X_train_pca, boston_y_train, cv=3))\n","lr_pca_scores.append(np.mean(lr_pca_scores))\n","lr_scores_df.loc[\"PCA Features\"] = lr_pca_scores\n","print(lr_scores_df.head())\n","\n","knn = KNeighborsRegressor()\n","\n","knn_scores_df = pd.DataFrame(columns=[\"R^2 on Fold 1\", \"R^2 on Fold 2\", \"R^2 on Fold 3\", \"Mean R^2\"])\n","print(\"\\nK-Nearest Neighbors Regression:\")\n","knn_scal_scores = list(cross_val_score(knn, boston_X_train_scal, boston_y_train, cv=3))\n","knn_scal_scores.append(np.mean(knn_scal_scores))\n","knn_scores_df.loc[\"No Feature Selection\"] = knn_scal_scores\n","knn_mi_scores = list(cross_val_score(knn, boston_X_train_mi, boston_y_train, cv=3))\n","knn_mi_scores.append(np.mean(knn_mi_scores))\n","knn_scores_df.loc[\"MI Features\"] = knn_mi_scores\n","knn_pca_scores = list(cross_val_score(knn, boston_X_train_pca, boston_y_train, cv=3))\n","knn_pca_scores.append(np.mean(knn_pca_scores))\n","knn_scores_df.loc[\"PCA Features\"] = knn_pca_scores\n","print(knn_scores_df.head())"]},{"cell_type":"markdown","metadata":{"id":"mRVBweA6-hjc"},"source":["Looking at the results, we see that linear regression performs similarily across the feature subsets but for KNN, we see that the model performs significantly better on the feature subset chosen using mutual information. Let's use GridSearchCV below to see if we can further improve its performance by adjusting the value of n_neighbors (the number of neighbors used)."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIZmFZ1k-hjc","executionInfo":{"status":"ok","timestamp":1694014486626,"user_tz":240,"elapsed":172,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"f7c8265d-91a1-405b-d709-c518e31fde6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best Params: {'n_neighbors': 3}\n","Best R^2 Score: 0.8166716460160099\n","{'mean_fit_time': array([0.00182748, 0.00059402, 0.00101638, 0.00044084, 0.00040627,\n","       0.00051546, 0.0004673 ]), 'std_fit_time': array([9.94682312e-04, 5.30481339e-05, 4.27722931e-04, 1.59740448e-05,\n","       2.07424164e-05, 1.35898590e-05, 4.60147858e-05]), 'mean_score_time': array([0.00183487, 0.00248253, 0.00257969, 0.00138366, 0.00134206,\n","       0.00151694, 0.00152409]), 'std_score_time': array([1.66893005e-05, 5.68032265e-04, 6.88076019e-04, 7.76052475e-05,\n","       6.27040863e-05, 1.63197517e-04, 1.10268593e-04]), 'param_n_neighbors': masked_array(data=[1, 2, 3, 4, 5, 6, 7],\n","             mask=[False, False, False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 2}, {'n_neighbors': 3}, {'n_neighbors': 4}, {'n_neighbors': 5}, {'n_neighbors': 6}, {'n_neighbors': 7}], 'split0_test_score': array([0.82304336, 0.80160726, 0.82015593, 0.80868266, 0.80492277,\n","       0.78887798, 0.7674301 ]), 'split1_test_score': array([0.77262725, 0.80728417, 0.81318736, 0.79076151, 0.77509845,\n","       0.76237471, 0.74496636]), 'mean_test_score': array([0.7978353 , 0.80444571, 0.81667165, 0.79972208, 0.79001061,\n","       0.77562634, 0.75619823]), 'std_test_score': array([0.02520806, 0.00283846, 0.00348429, 0.00896057, 0.01491216,\n","       0.01325163, 0.01123187]), 'rank_test_score': array([4, 2, 1, 3, 5, 6, 7], dtype=int32)}\n"]}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","knn = KNeighborsRegressor()\n","\n","# search over n_neighbors\n","param_grid = [{'n_neighbors': [1, 2, 3, 4, 5, 6, 7] }]\n","\n","cv_knn = GridSearchCV(knn, param_grid, cv=2)\n","cv_knn.fit(boston_X_train_mi, boston_y_train)\n","print(\"Best Params:\", cv_knn.best_params_)\n","print(\"Best R^2 Score:\", cv_knn.best_score_)\n","print(cv_knn.cv_results_)"]},{"cell_type":"markdown","metadata":{"id":"7GU5x5PD-hjc"},"source":["Using GridSearchCV, we see that the the value of n_neighbors resulting in the best performance was 3. Let's use this as our final model and apply it to our test set. We will evaluate the test set using Mean Squared Error (MSE) and Mean Absolute Error (MAE), two common measures of model performance for regression tasks. The formula for MSE is:\n","\n","$\\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i )^2$\n","\n","where $n$ is the number of data points, $y_i$ is the actual observation and $\\hat{y}_i$ is our prediction\n","\n","Similarly, the formula for MAE:\n","\n","$\\frac{1}{n}\\sum_{i=1}^n|(y_i - \\hat{y}_i )|$\n","\n","where $n$ is the number of data points, $y_i$ is the actual observation and $\\hat{y}_i$ is our prediction"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQ0kpfjL-hjc","executionInfo":{"status":"ok","timestamp":1694014524626,"user_tz":240,"elapsed":115,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"4240f09b-d2c3-4b94-a996-34f46a0628a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training R^2: 0.9391797935603915\n","Testing R^2: 0.8423629972031058\n","MSE: 12.876482939632547\n","MAE: 2.750656167979003\n"]}],"source":["from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","knn = KNeighborsRegressor(n_neighbors=3)\n","knn.fit(boston_X_train_mi, boston_y_train)\n","preds = knn.predict(boston_X_test_mi)\n","\n","print(\"Training R^2:\", knn.score(boston_X_train_mi, boston_y_train))\n","print(\"Testing R^2:\", knn.score(boston_X_test_mi, boston_y_test))\n","print(\"MSE:\", mean_squared_error(boston_y_test, preds))\n","print(\"MAE:\", mean_absolute_error(boston_y_test, preds))"]},{"cell_type":"markdown","metadata":{"id":"Qv_SY1Fi-hjc"},"source":["Nice! Our testing score increased to 0.84! Now, let's move onto the breast cancer data, a classification task..."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3rJSseu-hjc","executionInfo":{"status":"ok","timestamp":1694014747320,"user_tz":240,"elapsed":1193,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"7f5a26c5-411d-4c9c-f07f-48b38d93f909"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Classifier:\n","              Fold 1 Accuracy  Fold 2 Accuracy  Fold 3 Accuracy  Mean Accuracy\n","All Features         0.969925         0.939850         0.954545       0.954773\n","MI Features          0.954887         0.947368         0.946970       0.949742\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rf = RandomForestClassifier()\n","\n","rf_scores_df = pd.DataFrame(columns=[\"Fold 1 Accuracy\", \"Fold 2 Accuracy\", \"Fold 3 Accuracy\", \"Mean Accuracy\"])\n","print(\"Random Forest Classifier:\")\n","rf_scal_scores = list(cross_val_score(rf, cancer_X_train_scal, cancer_y_train, cv=3))\n","rf_scal_scores.append(np.mean(rf_scal_scores))\n","rf_scores_df.loc[\"All Features\"] = rf_scal_scores\n","rf_mi_scores = list(cross_val_score(rf, cancer_X_train_mi, cancer_y_train, cv=3))\n","rf_mi_scores.append(np.mean(rf_mi_scores))\n","rf_scores_df.loc[\"MI Features\"] = rf_mi_scores\n","print(rf_scores_df.head())\n","\n","\n","### YOUR CODE: find another classifier of your choice from scikit-learn & replicate the code above using it\n"]},{"cell_type":"code","source":["### EXAMPLE ANSWER\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier()\n","\n","knn_scores_df = pd.DataFrame(columns=[\"Fold 1 Accuracy\", \"Fold 2 Accuracy\", \"Fold 3 Accuracy\", \"Mean Accuracy\"])\n","print(\"\\nK-Nearest Neighbors Classifier:\")\n","knn_scal_scores = list(cross_val_score(knn, cancer_X_train_scal, cancer_y_train, cv=3))\n","knn_scal_scores.append(np.mean(knn_scal_scores))\n","knn_scores_df.loc[\"All Features\"] = knn_scal_scores\n","knn_mi_scores = list(cross_val_score(knn, cancer_X_train_mi, cancer_y_train, cv=3))\n","knn_mi_scores.append(np.mean(knn_mi_scores))\n","knn_scores_df.loc[\"MI Features\"] = knn_mi_scores\n","print(knn_scores_df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHn0WGcQL46H","executionInfo":{"status":"ok","timestamp":1694014863141,"user_tz":240,"elapsed":127,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"efff4fc0-161e-407e-ffc2-aebb419afc7c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","K-Nearest Neighbors Classifier:\n","              Fold 1 Accuracy  Fold 2 Accuracy  Fold 3 Accuracy  Mean Accuracy\n","All Features         0.962406         0.969925         0.969697       0.967343\n","MI Features          0.932331         0.932331         0.954545       0.939736\n"]}]},{"cell_type":"markdown","metadata":{"id":"SbKN9_34-hjd"},"source":["This is a good example to show that feature selection will not always improve performance. This is why we must experiment with different methods and models to optimize them for the task at hand."]},{"cell_type":"markdown","metadata":{"id":"LY5_YvcE-hjd"},"source":["For a lot of classification tasks, we will want to focus our evaluation on precision, recall and the f1-score. The classification_report function in scikit-learn makes it easy to do so. As a review:\n","\n","Precision = $\\frac{TP}{TP + FP}$, or the fraction of our positive predictions that are actually positive instances\n","\n","Recall = $\\frac{TP}{TP + FN}$, or the fraction of positive instances that we actually predict as positive\n","\n","F1-score = $2*\\: \\frac{precision\\: *\\: recall}{precision + recall}$, or the harmonic mean of precision and recall"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"MQYeUvoT-hjd","executionInfo":{"status":"ok","timestamp":1694015133471,"user_tz":240,"elapsed":203,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}}},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","### YOUR CODE: using your chosen model, compute predictions for the test data\n","### use these predictions to create the classification report\n"]},{"cell_type":"code","source":["### EXAMPLE ANSWER\n","\n","knn = KNeighborsClassifier()\n","knn.fit(cancer_X_train_scal, cancer_y_train)\n","preds = knn.predict(cancer_X_test_scal)\n","print(classification_report(cancer_y_test, preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-Y2fD4_MEbx","executionInfo":{"status":"ok","timestamp":1694015191945,"user_tz":240,"elapsed":121,"user":{"displayName":"Haoyang Ling","userId":"12767123704845142451"}},"outputId":"692079d4-0ced-4fec-87a9-e8a8ee85107c"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.98      0.93      0.95        68\n","           1       0.95      0.99      0.97       103\n","\n","    accuracy                           0.96       171\n","   macro avg       0.97      0.96      0.96       171\n","weighted avg       0.97      0.96      0.96       171\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4kHP6ibglwH5"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"video-info":{"admin":["localhost:8888"],"hub-user":"localhost:8888","path":"Labs>>Lab 1 - Machine Learning/Lab1_ML_Intro.ipynb","tpc":"localhost:8888@Labs>>Lab 1 - Machine Learning/Lab1_ML_Intro.ipynb"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}